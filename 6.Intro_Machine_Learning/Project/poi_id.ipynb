{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/envs/py2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load packages and scripts\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of employees: 146\n",
      "Number of POIs: 18\n",
      "Number of possible features: 20\n"
     ]
    }
   ],
   "source": [
    "# In[2]:\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    \n",
    "    \n",
    "#Exploring the dataset:\n",
    "\n",
    "#There are 146 people in the dictionary\n",
    "print (\"Number of employees: \" + str(len (data_dict)))\n",
    "\n",
    "#There are 18 POIs\n",
    "poi = []\n",
    "for k, v in data_dict.items():\n",
    "    if v[\"poi\"] == True:\n",
    "        poi.append(k)\n",
    "print (\"Number of POIs: \" + str(len (poi)))\n",
    "\n",
    "# There are 21 total features:\n",
    "possible_features = set()\n",
    "for v in data_dict.values():\n",
    "    for k in v:\n",
    "        if k != \"poi\":\n",
    "            possible_features.add(k)\n",
    "print (\"Number of possible features: \" + str(len(possible_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 1: Select features.\n",
    "    \n",
    "#Add all existing features to features list\n",
    "features_list = ['poi','salary'] \n",
    "\n",
    "for v in data_dict.values():\n",
    "    for k in v:\n",
    "        if k not in features_list:\n",
    "            features_list.append(str(k))\n",
    "\n",
    "#Remove 'email_address' - this string is a unique indentifier for each employee\n",
    "features_list.remove('email_address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 2: Remove outliers \n",
    "\n",
    "#Remove aggregate datapoint:\n",
    "del data_dict['TOTAL']\n",
    "\n",
    "#Remove non-employee datapoint:\n",
    "del data_dict [\"THE TRAVEL AGENCY IN THE PARK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Problems: ['BELFER ROBERT', 'BHATNAGAR SANJAY']\n",
      "Payments Problems: ['BELFER ROBERT', 'BHATNAGAR SANJAY']\n"
     ]
    }
   ],
   "source": [
    "### More outlier removal\n",
    "\n",
    "\n",
    "# Change NaNs to zeros. Although the featureformat function function would otherwise \n",
    "# perform this function, doing it prior makes it easier to  parse data for further outliers\n",
    "\n",
    "\n",
    "for k,v in data_dict.items():\n",
    "    for sub_k, sub_v in v.items():\n",
    "         if sub_v == \"NaN\": \n",
    "            v [sub_k] = 0   \n",
    "\n",
    "\n",
    "# Check for any cases where financial information does not add up correctly:\n",
    "\n",
    "total_payments_sum = [\n",
    "\"salary\",\n",
    "\"deferral_payments\",\n",
    "\"bonus\",\n",
    "\"expenses\",\n",
    "\"loan_advances\",\n",
    "\"other\",\n",
    "\"director_fees\",\n",
    "\"deferred_income\",\n",
    "\"long_term_incentive\"]\n",
    "\n",
    "total_stock_value_sum = [\n",
    "\"exercised_stock_options\",\n",
    "\"restricted_stock\",\n",
    "\"restricted_stock_deferred\"]\n",
    "\n",
    "\n",
    "total_payments_problems = []\n",
    "total_stock_problems = []\n",
    "\n",
    "for key, value in data_dict.items():\n",
    "    test = 0\n",
    "    for sub_k in value:\n",
    "        if sub_k in total_payments_sum:\n",
    "            test += value[sub_k]\n",
    "    if test == value[\"total_payments\"]:\n",
    "        continue\n",
    "    else:\n",
    "        total_payments_problems.append(key)\n",
    "                \n",
    "\n",
    "for key, value in data_dict.items():\n",
    "    test = 0\n",
    "    for sub_k in value:\n",
    "        if sub_k in total_stock_value_sum:\n",
    "            test += value[sub_k]\n",
    "    if test == value[\"total_stock_value\"]:\n",
    "        continue\n",
    "    else:\n",
    "        total_stock_problems.append(key)\n",
    "\n",
    "            \n",
    "print (\"Stock Problems: \" + str(total_stock_problems))\n",
    "print (\"Payments Problems: \" + str(total_payments_problems))\n",
    "\n",
    "\n",
    "\n",
    "#Since lists happen to be the same, can use either to remove further outliers:\n",
    "for employee in total_stock_problems:\n",
    "    del data_dict[employee]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "\n",
    "#Ratio values created for each subtotal\n",
    "for k,v in data_dict.items():\n",
    "    if v[\"total_payments\"] != 0 :\n",
    "        v[\"percent_salary\"] = float(float(v[\"salary\"])/ float(v[\"total_payments\"]))\n",
    "        v[\"percent_deferral_payments\"] = float(float(v[\"deferral_payments\"])/ float(v[\"total_payments\"]))\n",
    "        v[\"percent_bonus\"] = float(float(v[\"bonus\"])/ float(v[\"total_payments\"]))\n",
    "        v[\"percent_expenses\"] = float(float(v[\"expenses\"])/ float(v[\"total_payments\"]))\n",
    "        v[\"percent_loan_advances\"] = float(float(v[\"loan_advances\"])/ float(v[\"total_payments\"]))\n",
    "        v[\"percent_other\"] = float(float(v[\"other\"])/ float(v[\"total_payments\"]))\n",
    "        v[\"percent_director_fees\"] = float(float(v[\"director_fees\"])/ float(v[\"total_payments\"]))\n",
    "        v[\"percent_deferred_income\"] = float(float(v[\"deferred_income\"])/ float(v[\"total_payments\"]))\n",
    "        v[\"percent_long_term_incentive\"] = float(float(v[\"long_term_incentive\"])/ float(v[\"total_payments\"]))\n",
    "        \n",
    "    else:\n",
    "        v[\"percent_salary\"] = 0\n",
    "        v[\"percent_deferral_payments\"] = 0\n",
    "        v[\"percent_bonus\"] = 0\n",
    "        v[\"percent_expenses\"] = 0\n",
    "        v[\"percent_loan_advances\"] = 0 \n",
    "        v[\"percent_other\"] = 0\n",
    "        v[\"percent_director_fees\"] = 0\n",
    "        v[\"percent_deferred_income\"] = 0\n",
    "        v[\"percent_long_term_incentive\"] = 0\n",
    "        \n",
    "\n",
    "for k,v in data_dict.items():\n",
    "    if v[\"total_stock_value\"] != 0:\n",
    "        v[\"percent_exercised_stock_options\"] = float(float(v[\"exercised_stock_options\"])/ float(v[\"total_stock_value\"]))\n",
    "        v[\"percent_restricted_stock\"] = float(float(v[\"restricted_stock\"])/ float(v[\"total_stock_value\"]))\n",
    "        v[\"percent_restricted_stock_deferred\"] = float(float(v[\"restricted_stock_deferred\"])/ float(v[\"total_stock_value\"]))\n",
    "    else:\n",
    "        v[\"percent_exercised_stock_options\"] = 0\n",
    "        v[\"percent_restricted_stock\"] = 0\n",
    "        v[\"percent_restricted_stock_deferred\"] = 0\n",
    "\n",
    "# Re-run code to add new feature to feature_list:\n",
    "\n",
    "for v in data_dict.values():\n",
    "    for k in v:\n",
    "        if k not in features_list:\n",
    "            features_list.append(str(k))\n",
    "features_list.remove('email_address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 4: Create classifier\n",
    "\n",
    "# Import necessary packages and create objects for pipeline.\n",
    "\n",
    "#We will test two classifiers: Decision Tree and K Nearest Neighbor, and use - respectively - select K best\n",
    "#and PCA to reduce the number of features.\n",
    "\n",
    "\n",
    "###'scaling'###\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalar = MinMaxScaler()\n",
    "\n",
    "\n",
    "### \"princomp\" ###\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA ()\n",
    "\n",
    "\n",
    "####\"classifier_nn\":###\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "nn = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "####\"kbest\"###\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "#Varied independently selector with top 6 features produces best results\n",
    "selector = SelectKBest()\n",
    "\n",
    "\n",
    "####\"classifier_dt\":###\n",
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier(random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initial evaluation using standard split of data, 30% reserved for testing\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "    \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def split_test (clf, print_results = False):\n",
    "    features_train, features_test, labels_train, labels_test =     train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    clf = clf.fit(features_train, labels_train)\n",
    "    pred = clf.predict(features_test)\n",
    "    accuracy = round(accuracy_score (labels_test, pred), 3)\n",
    "    recall = round (recall_score (labels_test, pred), 3)\n",
    "    precision = round (precision_score (labels_test, pred), 3)\n",
    "    f1 = round (f1_score(labels_test, pred), 3)\n",
    "    if print_results == True:\n",
    "        counter = 0\n",
    "        for i in pred:\n",
    "            if i == 1.:\n",
    "                counter +=1\n",
    "\n",
    "        counter0 = 0\n",
    "        for i in labels_test:\n",
    "            if i == 1.:\n",
    "                counter0 +=1\n",
    "        print clf\n",
    "        print (\"total number of POIs predicted: \" + str(counter) +\"/\" + str(len(pred)))\n",
    "        print (\"real number of POIs \" + str(counter0) +\"/\" + str(len(labels_test)))\n",
    "        print (\"acc.: \" + str(accuracy))\n",
    "        print (\"recall: \" + str(recall))\n",
    "        print (\"prec. :\" + str(precision))\n",
    "        print (\"f1 :\" + str(f1))\n",
    "    else:\n",
    "        return (accuracy, recall, precision, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of results:\n",
      "{10: {'f1': 0.4, 'recall': 0.5, 'precision': 0.333, 'accuracy': 0.86}}\n",
      "{9: {'f1': 0.125, 'recall': 0.25, 'precision': 0.083, 'accuracy': 0.674}}\n",
      "{8: {'f1': 0.5, 'recall': 0.75, 'precision': 0.375, 'accuracy': 0.86}}\n",
      "{7: {'f1': 0.2, 'recall': 0.25, 'precision': 0.167, 'accuracy': 0.814}}\n",
      "{6: {'f1': 0.667, 'recall': 0.75, 'precision': 0.6, 'accuracy': 0.93}}\n",
      "{5: {'f1': 0.4, 'recall': 0.5, 'precision': 0.333, 'accuracy': 0.86}}\n",
      "{4: {'f1': 0.5, 'recall': 0.75, 'precision': 0.375, 'accuracy': 0.86}}\n",
      "{3: {'f1': 0.4, 'recall': 0.5, 'precision': 0.333, 'accuracy': 0.86}}\n",
      "{2: {'f1': 0.364, 'recall': 0.5, 'precision': 0.286, 'accuracy': 0.837}}\n",
      "{1: {'f1': 0.25, 'recall': 0.25, 'precision': 0.25, 'accuracy': 0.86}}\n",
      "optimal k: 6\n",
      "Pipeline(memory=None,\n",
      "     steps=[('kbest', SelectKBest(k=6, score_func=<function f_classif at 0x117c51a28>)), ('classifier_dt', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=123,\n",
      "            splitter='best'))])\n",
      "total number of POIs predicted: 5/43\n",
      "real number of POIs 4/43\n",
      "acc.: 0.93\n",
      "recall: 0.75\n",
      "prec. :0.6\n",
      "f1 :0.667\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline for Decision Tree classifier and find optimal number of features/k value in selectKbest.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "results_list = []\n",
    "n = 10\n",
    "best_f1_score = 0\n",
    "optimal_k = 0\n",
    "\n",
    "while n >= 1:\n",
    "    results = {}\n",
    "    results[n] = {\"accuracy\":0, \"recall\":0, \"precision\":0,\"f1\":0}\n",
    "    selector = SelectKBest(k = n)\n",
    "    clf_dt =  Pipeline(steps=[('kbest',selector), (\"classifier_dt\", dt)])\n",
    "    accuracy, recall, precision, f1 = split_test (clf_dt)\n",
    "    results[n][\"accuracy\"] = accuracy\n",
    "    results[n][\"recall\"] = recall\n",
    "    results[n][\"precision\"] = precision\n",
    "    results[n][\"f1\"] = f1\n",
    "    results_list.append (results)\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        optimal_k = n\n",
    "    n = n -1\n",
    "\n",
    "    \n",
    "#Overview of results with number of features from 1 - 10\n",
    "print (\"Overview of results:\")\n",
    "\n",
    "for i in results_list:\n",
    "    print i\n",
    "\n",
    "    \n",
    "    \n",
    "# With k set at 6, we get very good intial results from this algoritm:\n",
    "print \"optimal k: \" + str(optimal_k) \n",
    "\n",
    "selector = SelectKBest(k = optimal_k)\n",
    "clf_dt =  Pipeline(steps=[('kbest',selector), (\"classifier_dt\", dt)])\n",
    "\n",
    "\n",
    "split_test (clf_dt, print_results = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.145955349707767, 13.909428574104082, 12.719703253729154, 10.900125188189467, 10.636513552907001, 8.8050540798110113]\n",
      "[0.27423539142704234, 0.20769726809806424, 0.16848484848484852, 0.14726652838375873, 0.14096238991874666, 0.061353573687539527]\n"
     ]
    }
   ],
   "source": [
    "# Scores for 6 features selected by decision tree classifier : \n",
    "\n",
    "print sorted(selector.scores_, reverse = True) [0:6]\n",
    "\n",
    "print sorted(dt.feature_importances_, reverse = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of results:\n",
      "{10: {'f1': 0.4, 'recall': 0.25, 'precision': 1.0, 'accuracy': 0.93}}\n",
      "{9: {'f1': 0.4, 'recall': 0.25, 'precision': 1.0, 'accuracy': 0.93}}\n",
      "{8: {'f1': 0.4, 'recall': 0.25, 'precision': 1.0, 'accuracy': 0.93}}\n",
      "{7: {'f1': 0.4, 'recall': 0.25, 'precision': 1.0, 'accuracy': 0.93}}\n",
      "{6: {'f1': 0.0, 'recall': 0.0, 'precision': 0.0, 'accuracy': 0.907}}\n",
      "{5: {'f1': 0.0, 'recall': 0.0, 'precision': 0.0, 'accuracy': 0.907}}\n",
      "{4: {'f1': 0.4, 'recall': 0.25, 'precision': 1.0, 'accuracy': 0.93}}\n",
      "{3: {'f1': 0.4, 'recall': 0.25, 'precision': 1.0, 'accuracy': 0.93}}\n",
      "{2: {'f1': 0.571, 'recall': 0.5, 'precision': 0.667, 'accuracy': 0.93}}\n",
      "{1: {'f1': 0.4, 'recall': 0.25, 'precision': 1.0, 'accuracy': 0.93}}\n",
      "optimal n components : 2\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('princomp', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('classifier_nn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'))])\n",
      "total number of POIs predicted: 3/43\n",
      "real number of POIs 4/43\n",
      "acc.: 0.93\n",
      "recall: 0.5\n",
      "prec. :0.667\n",
      "f1 :0.571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/envs/py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Applications/anaconda/envs/py2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Repeat process for k nearest neighbor and pca. The algorithm performs best with 2 components; \n",
    "#although it does not score as well as the decision tree classifier, it performs very well using \n",
    "#the split method of validation. \n",
    "\n",
    "results_list = []\n",
    "n = 10\n",
    "best_f1_score = 0\n",
    "optimal_n_components = 0\n",
    "\n",
    "while n >= 1:\n",
    "    results = {}\n",
    "    results[n] = {\"accuracy\":0, \"recall\":0, \"precision\":0,\"f1\":0}\n",
    "    pca = PCA (n_components = n)\n",
    "    clf_nn =  Pipeline(steps=[('scaling',scalar),('princomp',pca), (\"classifier_nn\", nn)])\n",
    "    accuracy, recall, precision, f1 = split_test (clf_nn)\n",
    "    results[n][\"accuracy\"] = accuracy\n",
    "    results[n][\"recall\"] = recall\n",
    "    results[n][\"precision\"] = precision\n",
    "    results[n][\"f1\"] = f1\n",
    "    results_list.append (results)\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        optimal_n_components = n\n",
    "    n = n -1\n",
    "\n",
    "    \n",
    "#Overview of results with number of features from 1 - 10\n",
    "print (\"Overview of results:\") \n",
    "\n",
    "for i in results_list:\n",
    "    print i\n",
    "    \n",
    "    \n",
    "    \n",
    "print \"optimal n components : \" + str(optimal_n_components) \n",
    "\n",
    "pca = PCA (n_components = optimal_n_components)\n",
    "clf_nn =  Pipeline(steps=[('scaling',scalar),('princomp',pca), (\"classifier_nn\", nn)])\n",
    "\n",
    "split_test (clf_nn, print_results = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.20127772  0.17352578]\n"
     ]
    }
   ],
   "source": [
    "# Rankings for variances in PCA\n",
    "\n",
    "print (pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('princomp', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('classifier_nn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'))])\n",
      "\tAccuracy: 0.88520\tPrecision: 0.58528\tRecall: 0.47700\tF1: 0.52562\tF2: 0.49533\n",
      "\tTotal predictions: 15000\tTrue positives:  954\tFalse positives:  676\tFalse negatives: 1046\tTrue negatives: 12324\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('kbest', SelectKBest(k=6, score_func=<function f_classif at 0x117c51a28>)), ('classifier_dt', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=123,\n",
      "            splitter='best'))])\n",
      "\tAccuracy: 0.80853\tPrecision: 0.29434\tRecall: 0.31200\tF1: 0.30291\tF2: 0.30830\n",
      "\tTotal predictions: 15000\tTrue positives:  624\tFalse positives: 1496\tFalse negatives: 1376\tTrue negatives: 11504\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# However, further evaluating each algorithm using a cross method of validation, \n",
    "#only the k nearest neighbor passes a .3 test for precison and recall\n",
    "\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\tRecall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "\n",
    "def test_classifier (clf, folds = 1000, print_results = True):\n",
    "    '''Note: this function modified from tester.py included in course materials'''\n",
    "    from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "    data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "\n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "           \n",
    "    total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "    accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "    precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "    recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "    f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "    f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "    if print_results == True:\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    else:\n",
    "        return (total_predictions,accuracy,precision,recall,f1,f2)\n",
    "   \n",
    "test_classifier (clf_nn)\n",
    "test_classifier (clf_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 5: Tune classifier for best parameters:\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "sss = StratifiedShuffleSplit(100, random_state=42)\n",
    "\n",
    "\n",
    "params_nn = {'classifier_nn__n_neighbors': [1,3,5,7,9]}\n",
    "params_dt = {\"kbest__k\": [1,2,3,4,5,6,7,8],\n",
    "            \"classifier_dt__min_samples_split\":[2,3,4]}\n",
    "\n",
    "def tune_classifier (clf, params, score_function):\n",
    "    '''Note: this function adopted from forum page on Udacity\n",
    "    https://discussions.udacity.com/t/stratifiedshufflesplit-and-train-test-split/255763 '''\n",
    "    gs = GridSearchCV(clf, params, cv = sss, scoring = score_function)\n",
    "    for train_index, test_index in sss.split(features, labels):\n",
    "        features_train       = []\n",
    "        features_test        = []\n",
    "        labels_train        = []\n",
    "        labels_test         = []\n",
    "        for index in train_index:\n",
    "            features_train.append(features[index])\n",
    "            labels_train.append(labels[index])\n",
    "        for index in test_index:\n",
    "            features_test.append(features[index])\n",
    "            labels_test.append(labels[index])\n",
    "\n",
    "        gs.fit(features_train, labels_train)\n",
    "\n",
    "    clf = gs.best_estimator_\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the decision tree algorithm in Gridsearch (under Stratified Shuffle Split methods of vaidation)\n",
    "# returns a SelectKBest value of 5 and min_samples_split of three.\n",
    "import numpy as np\n",
    "with np.errstate(invalid='ignore'):\n",
    "    tuned_dt = tune_classifier(clf_dt, params_dt, \"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('kbest', SelectKBest(k=5, score_func=<function f_classif at 0x117c51a28>)), ('classifier_dt', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=3,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=123,\n",
      "            splitter='best'))])\n",
      "\tAccuracy: 0.81393\tPrecision: 0.28564\tRecall: 0.26350\tF1: 0.27412\tF2: 0.26765\n",
      "\tTotal predictions: 15000\tTrue positives:  527\tFalse positives: 1318\tFalse negatives: 1473\tTrue negatives: 11682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# These configurations improve the algorithm just enough to achieve a .3 score on precision and recall using the more \n",
    "# rigorous Stratified Shuffle Split for scoring.\n",
    "\n",
    "test_classifier (tuned_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The default configuration for KNearestNeighbor of n_neighbors = 5 happens to also be \n",
    "# the optimal setting for the algorithm\n",
    "\n",
    "tuned_nn  = tune_classifier(clf_nn, params_nn, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('princomp', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('classifier_nn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'))])\n",
      "\tAccuracy: 0.88520\tPrecision: 0.58528\tRecall: 0.47700\tF1: 0.52562\tF2: 0.49533\n",
      "\tTotal predictions: 15000\tTrue positives:  954\tFalse positives:  676\tFalse negatives: 1046\tTrue negatives: 12324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Thus the previous scores are not improved \n",
    "test_classifier (tuned_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of results:\n",
      "{10: {'f1': 0.213, 'recall': 0.135, 'f2': 0.158, 'precision': 0.514, 'accuracy': 0.868}}\n",
      "{9: {'f1': 0.252, 'recall': 0.165, 'f2': 0.191, 'precision': 0.543, 'accuracy': 0.87}}\n",
      "{8: {'f1': 0.193, 'recall': 0.121, 'f2': 0.142, 'precision': 0.474, 'accuracy': 0.865}}\n",
      "{7: {'f1': 0.214, 'recall': 0.137, 'f2': 0.16, 'precision': 0.494, 'accuracy': 0.866}}\n",
      "{6: {'f1': 0.121, 'recall': 0.073, 'f2': 0.087, 'precision': 0.347, 'accuracy': 0.858}}\n",
      "{5: {'f1': 0.174, 'recall': 0.107, 'f2': 0.127, 'precision': 0.461, 'accuracy': 0.864}}\n",
      "{4: {'f1': 0.192, 'recall': 0.124, 'f2': 0.144, 'precision': 0.422, 'accuracy': 0.861}}\n",
      "{3: {'f1': 0.368, 'recall': 0.284, 'f2': 0.313, 'precision': 0.522, 'accuracy': 0.87}}\n",
      "{2: {'f1': 0.526, 'recall': 0.477, 'f2': 0.495, 'precision': 0.585, 'accuracy': 0.885}}\n",
      "{1: {'f1': 0.19, 'recall': 0.134, 'f2': 0.152, 'precision': 0.324, 'accuracy': 0.847}}\n",
      "optimal n components : 2\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('princomp', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('classifier_nn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'))])\n",
      "\tAccuracy: 0.88520\tPrecision: 0.58528\tRecall: 0.47700\tF1: 0.52562\tF2: 0.49533\n",
      "\tTotal predictions: 15000\tTrue positives:  954\tFalse positives:  676\tFalse negatives: 1046\tTrue negatives: 12324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Since varying n_components for a pca operation within gridsearch takes a very long time to \n",
    "# compute, code below tests the number of components from 1 to 10. The code is the same as before\n",
    "# except the method of validation is different.\n",
    "\n",
    "results_list = []\n",
    "n = 10\n",
    "best_f1_score = 0\n",
    "optimal_n_components = 0\n",
    "\n",
    "while n >= 1:\n",
    "    results = {}\n",
    "    results[n] = {\"accuracy\":0, \"recall\":0, \"precision\":0,\"f1\":0, \"f2\":0}\n",
    "    pca = PCA (n_components = n)\n",
    "    clf_nn =  Pipeline(steps=[('scaling',scalar),('princomp',pca), (\"classifier_nn\", nn)])\n",
    "    total_predictions,accuracy,precision,recall,f1,f2 = test_classifier(clf_nn, print_results    = False)\n",
    "    results[n][\"accuracy\"] = round(accuracy,3)\n",
    "    results[n][\"recall\"] = round(recall, 3)\n",
    "    results[n][\"precision\"] = round(precision,3)\n",
    "    results[n][\"f1\"] = round(f1,3)\n",
    "    results[n][\"f2\"] = round(f2,3)\n",
    "    results_list.append (results)\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        optimal_n_components = n\n",
    "    n = n -1\n",
    "    \n",
    "\n",
    "    \n",
    "#Overview of results with number of features from 1 - 10\n",
    "print (\"Overview of results:\") \n",
    "\n",
    "for i in results_list:\n",
    "    print i\n",
    "    \n",
    "# Here again, 2 components for pca produces the best results for our classifer and outperforms\n",
    "#the decision tree algorithm.\n",
    "print \"optimal n components : \" + str(optimal_n_components) \n",
    "\n",
    "pca = PCA (n_components = optimal_n_components)\n",
    "clf_nn =  Pipeline(steps=[('scaling',scalar),('princomp',pca), (\"classifier_nn\", nn)])\n",
    "\n",
    "test_classifier (clf_nn, print_results = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump classifier, dataset, and features_list to reproduce results.\n",
    "\n",
    "\n",
    "clf = clf_nn\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('princomp', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('classifier_nn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'))])\n",
      "\tAccuracy: 0.84287\tPrecision: 0.09707\tRecall: 0.02150\tF1: 0.03520\tF2: 0.02546\n",
      "\tTotal predictions: 15000\tTrue positives:   43\tFalse positives:  400\tFalse negatives: 1957\tTrue negatives: 12600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As a final excercise, we can test the importance of the created ratio features to the final algorithm \n",
    "# by re-runing the same test without these features.\n",
    "\n",
    "original_features = []\n",
    "\n",
    "#Remove all features created in step 3 \n",
    "for feature in features_list:\n",
    "    feature = feature.strip()\n",
    "    if feature.find(\"percent\"):\n",
    "        original_features.append(feature)\n",
    "    \n",
    "        \n",
    "features_list = original_features\n",
    "test_classifier (clf_nn, print_results = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
